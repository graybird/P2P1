package com.neurogrid.simulation;

import java.util.Collection;
import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.Properties;
import java.util.Random;
import java.util.Vector;

import junit.framework.TestCase;
import junit.framework.TestSuite;

import com.neurogrid.simulation.root.ContentMessage;
import com.neurogrid.simulation.root.Document;
import com.neurogrid.simulation.root.Keyword;
import com.neurogrid.simulation.root.Network;
import com.neurogrid.simulation.root.NetworkParameters;
import com.neurogrid.simulation.root.Node;
import com.neurogrid.simulation.statistics.SearchStatistics;
// JUnitDoclet end import

/**
* Generated by JUnitDoclet, a tool provided by
* ObjectFab GmbH under LGPL.
* Please see www.junitdoclet.org, www.gnu.org
* and www.objectfab.de for informations about
* the tool, the licence and the authors.
*/


public class NeuroGridNetworkTest
// JUnitDoclet begin extends_implements
extends TestCase
// JUnitDoclet end extends_implements
{
  // JUnitDoclet begin class
  com.neurogrid.simulation.NeuroGridNetwork neurogridnetwork = null;
  // JUnitDoclet end class
  
  public NeuroGridNetworkTest(String name) {
    // JUnitDoclet begin method NeuroGridNetworkTest
    super(name);
    NeuroGridNetwork.init(System.getProperty("Log4jConfig"));
    // JUnitDoclet end method NeuroGridNetworkTest
  }
  
  public com.neurogrid.simulation.NeuroGridNetwork createInstance() throws Exception {
    // JUnitDoclet begin method testcase.createInstance
    return new com.neurogrid.simulation.NeuroGridNetwork();
    // JUnitDoclet end method testcase.createInstance
  }
  
  protected void setUp() throws Exception {
    // JUnitDoclet begin method testcase.setUp
    super.setUp();
    neurogridnetwork = createInstance();
    // JUnitDoclet end method testcase.setUp
  }
  
  protected void tearDown() throws Exception {
    // JUnitDoclet begin method testcase.tearDown
    neurogridnetwork = null;
    super.tearDown();
    // JUnitDoclet end method testcase.tearDown
  }
  
  public void testGetCvsInfo() throws Exception {
    // JUnitDoclet begin method getCvsInfo
    // JUnitDoclet end method getCvsInfo
  }
  
  public void testInit() throws Exception {
    // JUnitDoclet begin method init
    // JUnitDoclet end method init
  }
  
  public void testClearNetwork() throws Exception {
    // JUnitDoclet begin method clearNetwork
    // JUnitDoclet end method clearNetwork
  }
  
  public void testRefresh() throws Exception {
    // JUnitDoclet begin method refresh
    // JUnitDoclet end method refresh
  }
  
  public void testGetNetworkID() throws Exception {
    // JUnitDoclet begin method getNetworkID
    // JUnitDoclet end method getNetworkID
  }
  
  public void testToString() throws Exception {
    // JUnitDoclet begin method toString
    // JUnitDoclet end method toString
  }
  
  public void testAddNodes() throws Exception {
    // JUnitDoclet begin method addNodes
    // JUnitDoclet end method addNodes
  }

  public void testCreateNodes() throws Exception {
    // JUnitDoclet begin method createNodes
    
    HashMap x_nodes = neurogridnetwork.createNodes(10,5,new Random(888));
    assertTrue("10 nodes not created",x_nodes.size() == 10);
    Iterator x_iter = x_nodes.values().iterator();
    String x_temp = null;
    int x_no_honest = 0;
    while(x_iter.hasNext())
    {
      x_temp = (x_iter.next().getClass()).getName();
      if(x_temp.equals("com.neurogrid.simulation.NeuroGridNode"))
        x_no_honest++;
      //System.out.println(x_temp);
    }
    
    assertTrue("not 5 honest nodes created",x_no_honest==5);
    assertTrue("not 10 nodes in the network",neurogridnetwork.getNoNodes()==10);
    assertTrue("not 5 honest nodes in the network",neurogridnetwork.getNoHonestNodes()==5);
    
    //assertTrue("static table doesn't have 5 nodes",Node.o_nodes.size()==5);
    // JUnitDoclet end method createNodes
  }
  
  
  public void testCreateKeywords() throws Exception {
    // JUnitDoclet begin method createKeywords
    
    HashMap x_keywords = neurogridnetwork.createKeywords(5);
    assertTrue("5 keywords not created",x_keywords.size() == 5);
    Iterator x_iter = x_keywords.values().iterator();
    String x_temp = null;
    while(x_iter.hasNext())
    {
      x_temp = (x_iter.next().getClass()).getName();
      //System.out.println(x_temp);
      assertTrue("not simple Keywords",x_temp.equals("com.neurogrid.simulation.SimpleKeyword"));
    }
    
    // I'm conflicted about whether keywords should be in the network
    // or stay as static items in the Keyword class ....?
    
    //assertTrue("static table doesn't have 5 keywords",Keyword.o_keywords.size()==5);
    // should we be clearing the network each time ???? FIXXXXXXXX
    // JUnitDoclet end method createKeywords
  }
  
  public void testCreateNetwork() throws Exception {
    // JUnitDoclet begin method createNetwork
    
    // creating the network involves
    // 1. making some nodes
    // 2. making some keywords and documents
    // 3. placing some of this content in the nodes
    // 4. generating connections
    // 5. generate knowledge .... 
    
    String x_sim_type = "NeuroGrid";

    int x_no_keywords = 15;
    int x_no_documents = 14;
    int x_no_keywords_per_document = 2;
    int x_no_nodes = 3;
    int x_no_honest_nodes = 1;
    int x_no_documents_per_node = 6;
    int x_max_knowledge_per_node = 7;
    int x_no_connections_per_node = 8;
    int x_max_connections_per_node = 9;
  
    int x_start_TTL = 10;
  
    int x_max_forwarding_degree = 11;
    int x_min_forwarding_degree = 12;
    int x_internal_loop = 13;
    int x_probe_loop = 14;
    int x_no_probes = 15;
    int x_growth_loop = 16;
    int x_stats_loop = 17;
  
    boolean x_ring_topology = true;
    boolean x_reciprocal_connections = false;
    boolean x_zipf_distribution = true;
    boolean x_node_doc_zipf_distribution = false;
    boolean x_random_searches = true;
    boolean x_random_forwarding = false;
    boolean x_applet = true;
 
    
    Properties x_properties = new Properties();
    
    x_properties.setProperty(NetworkParameters.SIMULATION_TYPE,x_sim_type);
    x_properties.setProperty(NetworkParameters.NO_KEYWORDS,Integer.toString(x_no_keywords));
    x_properties.setProperty(NetworkParameters.NO_DOCUMENTS,Integer.toString(x_no_documents));
    x_properties.setProperty(NetworkParameters.NO_KEYWORDS_PER_DOCUMENT,Integer.toString(x_no_keywords_per_document));
    x_properties.setProperty(NetworkParameters.NO_NODES,Integer.toString(x_no_nodes));
    x_properties.setProperty(NetworkParameters.NO_HONEST_NODES,Integer.toString(x_no_honest_nodes));
    x_properties.setProperty(NetworkParameters.NO_DOCUMENTS_PER_NODE,Integer.toString(x_no_documents_per_node));
    x_properties.setProperty(NetworkParameters.MAX_KNOWLEDGE_PER_NODE,Integer.toString(x_max_knowledge_per_node));
    x_properties.setProperty(NetworkParameters.NO_CONNECTIONS_PER_NODE,Integer.toString(x_no_connections_per_node));
    x_properties.setProperty(NetworkParameters.MAX_CONNECTIONS_PER_NODE,Integer.toString(x_max_connections_per_node));
    x_properties.setProperty(NetworkParameters.START_TTL,Integer.toString(x_start_TTL));
    x_properties.setProperty(NetworkParameters.MAX_FORWARDING_DEGREE,Integer.toString(x_max_forwarding_degree));
    x_properties.setProperty(NetworkParameters.MIN_FORWARDING_DEGREE,Integer.toString(x_min_forwarding_degree));
    x_properties.setProperty(NetworkParameters.INTERNAL_LOOP,Integer.toString(x_internal_loop));
    x_properties.setProperty(NetworkParameters.PROBE_LOOP,Integer.toString(x_probe_loop));
    x_properties.setProperty(NetworkParameters.NO_PROBES,Integer.toString(x_no_probes));
    x_properties.setProperty(NetworkParameters.GROWTH_LOOP,Integer.toString(x_growth_loop));
    x_properties.setProperty(NetworkParameters.STATS_LOOP,Integer.toString(x_stats_loop));
    x_properties.setProperty(NetworkParameters.RING_TOPOLOGY,Boolean.toString(x_ring_topology));
    x_properties.setProperty(NetworkParameters.RECIPROCAL_CONNECTIONS,Boolean.toString(x_reciprocal_connections));
    x_properties.setProperty(NetworkParameters.DOC_KEYWORD_ZIPF_DISTRIBUTION,Boolean.toString(x_zipf_distribution));
    x_properties.setProperty(NetworkParameters.NODE_DOC_ZIPF_DISTRIBUTION,Boolean.toString(x_node_doc_zipf_distribution));
    x_properties.setProperty(NetworkParameters.RANDOM_SEARCHES,Boolean.toString(x_random_searches));
    x_properties.setProperty(NetworkParameters.RANDOM_FORWARDING,Boolean.toString(x_random_forwarding));
    x_properties.setProperty(NetworkParameters.APPLET,Boolean.toString(x_applet));
    
    // really we need a series of Network Parameters because not all parameters
    // are relevant for all simulations - could have NeuroGridParameters etc.
    
    //FileInputStream x_input = new FileInputStream("./conf/NEUROGRID.properties");
    //o_properties.load(x_input);
    //x_input.close();
    
    NetworkParameters x_network_parameters = new NetworkParameters();
    x_network_parameters.parse(x_properties);
    
    neurogridnetwork.createNetwork(x_network_parameters,new Random(555));
    
    assertTrue("Network has incorrect number of nodes",
               neurogridnetwork.getNoNodes()==x_no_nodes);
               
    assertTrue("Network has incorrect number of honest nodes",
               neurogridnetwork.getNoHonestNodes()==x_no_honest_nodes);
               
               
    // need to test we have correct number of honest nodes
    //FIXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
    // there are other things like number of docs per node, but no time for that now           
    
    assertTrue("Network has wrong kind of topology",
               neurogridnetwork.o_ring_topology==x_ring_topology);
    assertTrue("reciprocal connections wrong",
               neurogridnetwork.o_reciprocal_connections==x_reciprocal_connections);
    assertTrue("o_zipf_distribution wrong",
               neurogridnetwork.o_zipf_distribution==x_zipf_distribution);
    assertTrue("o_node_doc_zipf_distribution wrong",
               neurogridnetwork.o_node_doc_zipf_distribution==x_node_doc_zipf_distribution);
    assertTrue("o_random_searches wrong",
               neurogridnetwork.o_random_searches==x_random_searches);
    assertTrue("o_random_forwarding wrong",
               neurogridnetwork.o_random_forwarding==x_random_forwarding);
               
    assertTrue("o_max_connections_per_node wrong",
               neurogridnetwork.o_max_connections_per_node==x_max_connections_per_node);
    assertTrue("o_max_knowledge_per_node wrong",
               neurogridnetwork.o_max_knowledge_per_node==x_max_knowledge_per_node);
    assertTrue("o_max_forwarding_degree wrong",
               neurogridnetwork.o_max_forwarding_degree==x_max_forwarding_degree);
    assertTrue("o_min_forwarding_degree wrong",
               neurogridnetwork.o_min_forwarding_degree==x_min_forwarding_degree);
    assertTrue("o_applet wrong",
               Network.o_applet==x_applet);



    
    // now we need to assess the network/documents etc...
    // feels like testing would be much easier if all the different objects where associated 
    // with the network and got created from scratch each time, and any accessing of these
    // objects came through queries on the network itself ...
    
    // so next step would be to modify createNetwork and then run a complete test of that
    // before moving on to search network and associated test ....
    // JUnitDoclet end method createNetwork
  }
  
  public void testCreateKeywordsAndDocuments() throws Exception {
    // JUnitDoclet begin method createKeywordsAndDocuments
    // JUnitDoclet end method createKeywordsAndDocuments
  }
  
  public void testSetSendingMessageFalse() throws Exception {
    // JUnitDoclet begin method setSendingMessageFalse
    // JUnitDoclet end method setSendingMessageFalse
  }
  
  public void testHasKeyword() throws Exception {
    // JUnitDoclet begin method setSendingMessageFalse
    
    // need to create documents with specific keywords and add them to 
    // the network
    try
    {
      neurogridnetwork.hasKeyword(null);
      fail("Failed to throw exception when checking for null content");	
    }
    catch(Exception e){}

    Keyword[] x_keywords = new SimpleKeyword[3];
    Keyword[] x_keywords2 = new SimpleKeyword[3];
    
    for(int i=0;i<3;i++)
    {
      x_keywords[i] = new SimpleKeyword();	
      x_keywords2[i] = new SimpleKeyword();	
    }	

    SimpleDocument x_doc = new SimpleDocument(x_keywords);
    SimpleDocument x_doc2 = new SimpleDocument(x_keywords2);
    
    NeuroGridNode x_node = new NeuroGridNode(neurogridnetwork,new Random(888));
    
    x_node.addContent(x_doc);

    for(int i=0;i<3;i++)
    {    
      assertTrue("local keyword not indicated",neurogridnetwork.hasKeyword(x_keywords[i]) == 1);    
      assertTrue("absent keyword identified as local",neurogridnetwork.hasKeyword(x_keywords2[i]) == 0); 
    }      
    // JUnitDoclet end method setSendingMessageFalse
  }
  
  public void testGetRandomSearchKeywords() throws Exception {
    // JUnitDoclet begin method getRandomSearchKeywords
    
    HashMap x_keys = neurogridnetwork.createKeywords(5);
    int x_no_docs = 10;
    int x_no_keywords_per_doc = 3;
    final int SEED = 999;
    Random x_random = new Random(SEED);
    
    HashMap x_docs = neurogridnetwork.createDocuments(x_no_docs,
                                                     x_no_keywords_per_doc,
                                                     false,
                                                     x_random);
   
    
    HashMap x_nodes = neurogridnetwork.createNodes(10,10,new Random(888));
    neurogridnetwork.generateContent(x_nodes,
                                    x_docs,
                                    x_docs,
                                    3,
                                    false,
                                    x_random);
                                    
    Iterator x_node_iter = x_nodes.values().iterator();

    Keyword[] x_keywords = null;
    int x_no_keywords = 3;
    Node x_node = null;
    
    // not sure if there is an effective way to test that 
    // the different picks really are random ......
    
    while(x_node_iter.hasNext())
    {
      x_node = (Node)(x_node_iter.next());
      x_keywords = neurogridnetwork.getRandomSearchKeywords(x_no_keywords,x_random);
    
      // would like to get keywords that do exist in the network ....
      // and that are associated with the search start location ....
    
      for(int i=0;i<x_no_keywords;i++)
      {
      	System.out.println(x_keywords[i].getKeywordID());
        assertTrue("keyword not in network",neurogridnetwork.hasKeyword(x_keywords[i])>0);
        assertTrue("keywords not unique",x_keywords[i] != x_keywords[(i+1)%x_no_keywords]);
      } 
      
    }
    
    // would like to get keywords that do exist in the network ....

    // JUnitDoclet end method getRandomSearchKeywords
  }
  
  public void testGetAssociatedSearchKeywords() throws Exception {
    // JUnitDoclet begin method getAssociatedSearchKeywords

    neurogridnetwork.createKeywords(5);
    int x_no_docs = 10;
    int x_no_keywords_per_doc = 3;
    final int SEED = 999;
    Random x_random = new Random(SEED);
    
    HashMap x_docs = neurogridnetwork.createDocuments(x_no_docs,
                                                     x_no_keywords_per_doc,
                                                     false,
                                                     x_random);
   
    
    HashMap x_nodes = neurogridnetwork.createNodes(10,10,new Random(888));
    neurogridnetwork.generateContent(x_nodes,
                                    x_docs,
                                    x_docs,
                                    3,
                                    false,
                                    x_random);
                                    
    Iterator x_node_iter = x_nodes.values().iterator();


    Keyword[] x_keywords = null;
    int x_no_keywords = 3;
    Node x_node = null;
    
    // not sure if there is an effective way to test that 
    // the different picks really are random ......
    
    while(x_node_iter.hasNext())
    {
      x_node = (Node)(x_node_iter.next());
      x_keywords = neurogridnetwork.getAssociatedSearchKeywords(x_no_keywords,x_node,x_random);
    
      // would like to get keywords that do exist in the network ....
      // and that are associated with the search start location ....
    
      for(int i=0;i<x_no_keywords;i++)
      {
      	//System.out.println(x_keywords[i].getKeywordID());
        assertTrue("keyword not associated with node",x_node.hasKeyword(x_keywords[i])>0);
        assertTrue("keywords not unique",x_keywords[i] != x_keywords[(i+1)%x_no_keywords]);
      } 
      
    }
    // JUnitDoclet end method getAssociatedSearchKeywords
  }
  
  public void testGetRandomSearchStart() throws Exception {
    // JUnitDoclet begin method getRandomSearchStart

    neurogridnetwork.createKeywords(5);
    int x_no_docs = 10;
    int x_no_keywords_per_doc = 3;
    final int SEED = 999;
    Random x_random = new Random(SEED);
    
    HashMap x_docs = neurogridnetwork.createDocuments(x_no_docs,
                                                     x_no_keywords_per_doc,
                                                     false,
                                                     x_random);
    Iterator x_doc_iter = x_docs.values().iterator();
    
    HashMap x_nodes = neurogridnetwork.createNodes(10,10,x_random);
    neurogridnetwork.generateContent(x_nodes,
                                    x_docs,
                                    x_docs,
                                    3,
                                    false,
                                    x_random);

    Vector x_vec = Node.getVectorFromMap(x_nodes);
    //neurogridnetwork.generateRandomTopology(x_vec,x_vec,2,true);
    Node x_node = null;
    Document x_doc = null;
    while(x_doc_iter.hasNext())
    {
      x_doc = (Document)(x_doc_iter.next());
      x_node = neurogridnetwork.getRandomSearchStart(x_doc,x_random);
      assertTrue("selected node "+x_node.getNodeID()+" contains target document",!x_node.hasContent(x_doc));
    }
    
    // JUnitDoclet end method getRandomSearchStart
  }
  
  public void testGetRandomSearchStart2() throws Exception {
    // JUnitDoclet begin method getRandomSearchStart

    neurogridnetwork.createKeywords(5);
    int x_no_docs = 10;
    int x_no_keywords_per_doc = 3;
    final int SEED = 999;
    Random x_random = new Random(SEED);
    
    HashMap x_docs = neurogridnetwork.createDocuments(x_no_docs,
                                                     x_no_keywords_per_doc,
                                                     false,
                                                     x_random);
    Iterator x_doc_iter = x_docs.values().iterator();
    
    HashMap x_nodes = neurogridnetwork.createNodes(10,5,x_random);
    neurogridnetwork.generateContent(x_nodes,
                                    x_docs,
                                    x_docs,
                                    3,
                                    false,
                                    x_random);

    Vector x_vec = Node.getVectorFromMap(x_nodes);
    HashSet x_excludes = new HashSet();
    //neurogridnetwork.generateRandomTopology(x_vec,x_vec,2,true);
    Node x_node = neurogridnetwork.getRandomSearchStart(x_excludes,
                                                        "com.neurogrid.simulation.NeuroGridNode",
                                                        x_random);
    assertTrue("selected node "+x_node.getNodeID()+" is not NeuroGridNode",
               x_node.getClass().getName().equals("com.neurogrid.simulation.NeuroGridNode"));
               
    x_excludes.add(x_node);

    Node x_node2 = neurogridnetwork.getRandomSearchStart(x_excludes,
                                                        "com.neurogrid.simulation.NeuroGridNode",
                                                        x_random);
    assertTrue("selected node "+x_node2.getNodeID()+" is not NeuroGridNode",
               x_node2.getClass().getName().equals("com.neurogrid.simulation.NeuroGridNode"));
    assertTrue("selected node "+x_node2.getNodeID()+" is part of excluded list",
               x_node2 != x_node);

    
    x_excludes.add(x_node2);
    
    Node x_node3 = neurogridnetwork.getRandomSearchStart(x_excludes,
                                                         "com.neurogrid.simulation.AdversarialNode",
                                                         x_random);
    assertTrue("selected node "+x_node3.getNodeID()+" is not AdversarialNode",
               x_node3.getClass().getName().equals("com.neurogrid.simulation.AdversarialNode"));
    assertTrue("selected node "+x_node2.getNodeID()+" is part of excluded list",
               x_node3 != x_node);
    assertTrue("selected node "+x_node2.getNodeID()+" is part of excluded list",
               x_node3 != x_node2);

    x_excludes.add(x_node3);

    Node x_node4 = neurogridnetwork.getRandomSearchStart(x_excludes,
                                                         "com.neurogrid.simulation.AdversarialNode",
                                                         x_random);
    assertTrue("selected node "+x_node4.getNodeID()+" is not AdversarialNode",
               x_node4.getClass().getName().equals("com.neurogrid.simulation.AdversarialNode"));
    assertTrue("selected node "+x_node4.getNodeID()+" is part of excluded list",
               x_node4 != x_node3);
    assertTrue("selected node "+x_node2.getNodeID()+" is part of excluded list",
               x_node4 != x_node2);
    assertTrue("selected node "+x_node2.getNodeID()+" is part of excluded list",
               x_node4 != x_node);

    
    // JUnitDoclet end method getRandomSearchStart
  }
  
  public void testGetAssociatedSearchTarget() throws Exception {
    // JUnitDoclet begin method getAssociatedSearchTarget
    
    neurogridnetwork.createKeywords(5);
    int x_no_docs = 10;
    int x_no_keywords_per_doc = 3;
    final int SEED = 999;
    Random x_random = new Random(SEED);
    
    HashMap x_docs = neurogridnetwork.createDocuments(x_no_docs,
                                                     x_no_keywords_per_doc,
                                                     false,
                                                     x_random);
    
    HashMap x_nodes = neurogridnetwork.createNodes(10,10,new Random(888));
    Iterator x_node_iter = x_nodes.values().iterator();

    neurogridnetwork.generateContent(x_nodes,
                                    x_docs,
                                    x_docs,
                                    3,
                                    false,
                                    x_random);

    Vector x_vec = Node.getVectorFromMap(x_nodes);
    //neurogridnetwork.generateRandomTopology(x_vec,x_vec,2,true);
    Node x_node = null;
    Document x_doc = null;
    int x_sum = 0;
    while(x_node_iter.hasNext())
    {
      x_node = (Node)(x_node_iter.next());
      x_doc = neurogridnetwork.getAssociatedSearchTarget(x_node,x_random);
      assertTrue("returned document is null",x_doc != null);
      // maybe doc is allowed to be null
      assertTrue("selected node "+x_node.getNodeID()+" contains target document",!x_node.hasContent(x_doc));
      // should check that search target does share a keyword in common with the contents of the local node 
      x_sum = 0;
      for(int i=0;i<x_no_keywords_per_doc;i++)
      {
      	// next line gives an odd out of bounds exception in a testall -
      	// presumably some document is not being given its full complement
      	// of keywords
      	// FIXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
      	if(x_node.hasKeyword(x_doc.getKeywords()[i]) >= 1)
      	{
      	  x_sum++;
      	}
      	assertTrue("no associated keywords in start location",x_sum > 0);
      	// this will fail sometimes - would need some work for additional checking
      }
    }
      
    
    // JUnitDoclet end method getAssociatedSearchTarget
  }
  
  public void testFuzzySearchNetwork() throws Exception {
    // JUnitDoclet begin method searchNetwork

    HashMap x_keywords = neurogridnetwork.createKeywords(10);
    int x_no_docs = 5;
    int x_no_keywords_per_doc = 3;
    final int SEED = 999;
    
    // feels like we should be specifying the type of distribution through an argument rather than
    // a global variable
    // also feels like we should be handing back a set of documents to check ....
    
    HashMap x_docs = neurogridnetwork.createDocuments(x_no_docs,
                                                     x_no_keywords_per_doc,
                                                     false,
                                                     new Random(SEED));
    Iterator x_doc_iter = x_docs.values().iterator();
    
    HashMap x_nodes = neurogridnetwork.createNodes(5,5,new Random(888));
    Iterator x_node_iter = x_nodes.values().iterator();
    neurogridnetwork.generateContent(x_nodes,
                                    x_docs,
                                    x_docs,
                                    3,
                                    false,
                                    new Random(SEED));
    // feels like generate content should take Documents as an argument ...
    // issue is that we can have several different distributions of documents
    // i.e. they are not associated with the network in the way that nodes are
    // one could imagine a DocumentCollection? Corpus? class that would represent
    // a set of documents/files ....
    Vector x_vec = Node.getVectorFromMap(x_nodes);
    neurogridnetwork.generateRingTopology(x_vec,2);
    
    SearchStatistics x_search_stats = null;
    Document x_doc = null;
    Node x_node = null;
    int x_TTL = 5;
    int[] x_stats = new int[6];    
    x_stats[ACTIVATED_NODES] = 0;
    x_stats[POSSIBLE_TARGETS] = 0;
    x_stats[MESSAGE_TRANSFERS] = 0;
    x_stats[TTL_FIRST_MATCH] = -1;
    x_stats[NO_MATCHES] = 0;
    x_stats[NO_FALSE_MATCHES] = 0;
    HashSet x_checked_nodes = new HashSet(); 
    HashSet x_possible_targets = new HashSet(); 
    LinkedList x_unchecked_nodes = new LinkedList(); 
    ContentMessage x_message = null;
    
    Keyword[] x_keys = neurogridnetwork.getRandomSearchKeywords(x_no_keywords_per_doc,new Random(SEED));
        
    //while(x_iter.hasNext())    
    //{
      x_doc = (Document)x_doc_iter.next();
      x_node = (Node)x_node_iter.next();
      x_message = new FuzzyContentMessage(x_TTL, 
                                   x_keys, 
                                   x_node);
                                   
      x_search_stats = neurogridnetwork.searchNetwork(x_message);

      // seems to require me to have a store of which items to search
      // but also to associate some flag with each of them to indicate 
      // whether something was discovered above it in the search tree                                              
                                             
      System.out.println("starting node: "+x_node.getNodeID());
                
      checkFuzzyNode(new SearchHolder(x_node,
                                 x_node,
                                 null,
                                 x_keys,
                                 0,
                                 x_TTL,
                                 x_stats,
                                 x_checked_nodes,
                                 x_possible_targets,
                                 false,
                                 x_unchecked_nodes));
      
      SearchHolder x_holder = null;
      
      do
      {
        x_holder = (SearchHolder)(x_unchecked_nodes.removeLast());

        System.out.println("x_unchecked_nodes.size(): "+x_unchecked_nodes.size());
        System.out.println("x_node: "+x_holder.o_node.getNodeID());
        
        checkFuzzyNode(x_holder);
                     
      }while(x_unchecked_nodes.size() > 0);
      
        System.out.println("x_stats[ACTIVATED_NODES]: "+x_stats[ACTIVATED_NODES]);
        System.out.println("x_stats[POSSIBLE_TARGETS]: "+x_stats[POSSIBLE_TARGETS]);
        System.out.println("x_stats[MESSAGE_TRANSFERS]: "+x_stats[MESSAGE_TRANSFERS]);
        System.out.println("x_stats[TTL_FIRST_MATCH]: "+x_stats[TTL_FIRST_MATCH]);
        System.out.println("x_stats[NO_MATCHES]: "+x_stats[NO_MATCHES]);
        System.out.println("x_stats[NO_FALSE_MATCHES]: "+x_stats[NO_FALSE_MATCHES]);

      
      assertTrue("No. activated nodes incorrect",x_search_stats.getNoActivatedNodes()==x_stats[ACTIVATED_NODES]);
      assertTrue("No. possible targets incorrect",x_search_stats.getNoPossibleTargets()==x_stats[POSSIBLE_TARGETS]);
      assertTrue("No. message transfers incorrect",x_search_stats.getNoMessageTransfers()==x_stats[MESSAGE_TRANSFERS]);
      assertTrue("first match ttl incorrect",x_search_stats.getTTLOfFirstMatch()==x_stats[TTL_FIRST_MATCH]);
      assertTrue("No. matches incorrect",x_search_stats.getNoMatches()==x_stats[NO_MATCHES]);
      assertTrue("No. false matches incorrect",x_search_stats.getNoFalseMatches()==x_stats[NO_FALSE_MATCHES]);
  
      // makes more sense to do refresh test here ...
      
      neurogridnetwork.refresh();

      x_message = new FuzzyContentMessage(x_TTL, 
                                   x_keys, 
                                   x_node);      
      SearchStatistics x_search_stats2 = neurogridnetwork.searchNetwork(x_message);

      // would love to check how many objects we were increasing by after each loop

 
      assertTrue("post-refresh: No. activated nodes incorrect",
                 x_search_stats.getNoActivatedNodes()==x_search_stats2.getNoActivatedNodes());
      assertTrue("post-refresh: No. possible targets incorrect",
                 x_search_stats.getNoPossibleTargets()==x_search_stats2.getNoPossibleTargets());
      assertTrue("post-refresh: No. message transfers incorrect",
                 x_search_stats.getNoMessageTransfers()==x_search_stats2.getNoMessageTransfers());
      assertTrue("post-refresh: first match ttl incorrect",
                 x_search_stats.getTTLOfFirstMatch()==x_search_stats2.getTTLOfFirstMatch());
      assertTrue("post-refresh: No. matches incorrect",
                 x_search_stats.getNoMatches()==x_search_stats2.getNoMatches());
                 

 
      // so we should be checking the stats coming back from the search ...
      /*
      x_search_stats.getNoMatches()
      x_search_stats.getTTLOfFirstMatch()
      x_search_stats.getNoMessageTransfers()
      x_search_stats.getNoActivatedNodes()
      x_search_stats.getNoPossibleTargets() */
    //}
    

    
    // would be nice to back all of this up to cvs - or even to the linux box ...
    
    
    // JUnitDoclet end method searchNetwork
  }
  
  public void testSearchHonestNetwork() throws Exception {
    // JUnitDoclet begin method searchNetwork

   
    SearchStatistics x_stats = searchNetwork(5,5);
    assertTrue("false matches in an honest network",x_stats.getNoFalseMatches() ==0);
    
    // JUnitDoclet end method searchNetwork
  }
  public void testSearchMixedNetwork() throws Exception {
    // JUnitDoclet begin method searchNetwork

    SearchStatistics x_stats = searchNetwork(5,3);
    assertTrue("no false matches in dishonest network",x_stats.getNoFalseMatches()!=0);    
    // problem is that circumstance can contrive such that 
    // adversarial nodes do have document in question ...

    // need to remove any unaccounted for randomness, and confirm that
    // knowledge bases are being properly updated --> I guess that will only 
    // happen when a neurogrid node does a search that finds the correct item
    // in a target location ...
    
    // need to confirm knowledge updates as parts of searches ...
    // need stats from AdvancedSimulation ... checking for changes in connectivity?
    
    // in terms of results I presume that searches starting in NG nodes will show
    // better results
    // I do need to confirm how the simulation is conducting searches
        
    // also will adversarial nodes update their knowledge bases? - perhaps they don't have
    // any ..? actually they do since the knowledge base is part of the Node class

    // JUnitDoclet end method searchNetwork
  }
  
  public SearchStatistics searchNetwork(int p_no_nodes,int p_no_honest_nodes) 
  throws Exception {
    // JUnitDoclet begin method searchNetwork

    neurogridnetwork.createKeywords(5);
    int x_no_docs = 15;
    int x_no_keywords_per_doc = 3;
    final int SEED = 555;
    Random x_random = new Random(SEED);
    
    // feels like we should be specifying the type of distribution through an argument rather than
    // a global variable
    // also feels like we should be handing back a set of documents to check ....
    
    HashMap x_docs = neurogridnetwork.createDocuments(x_no_docs,
                                                     x_no_keywords_per_doc,
                                                     false,
                                                     x_random);
    Iterator x_doc_iter = x_docs.values().iterator();

    neurogridnetwork.o_max_connections_per_node=2;
    System.out.println("neurogridnetwork.o_max_connections_per_node="+neurogridnetwork.o_max_connections_per_node);

    
    HashMap x_nodes = neurogridnetwork.createNodes(p_no_nodes,p_no_honest_nodes,x_random);
    assertTrue("incorrect no. nodes in the network",neurogridnetwork.getNoNodes()==p_no_nodes);
    assertTrue("incorrect no. honest nodes in the network",neurogridnetwork.getNoHonestNodes()==p_no_honest_nodes);

    Iterator x_node_iter = x_nodes.values().iterator();
    neurogridnetwork.generateContent(x_nodes,
                                    x_docs,
                                    x_docs,
                                    3,
                                    false,
                                    x_random);
    // feels like generate content should take Documents as an argument ...
    // issue is that we can have several different distributions of documents
    // i.e. they are not associated with the network in the way that nodes are
    // one could imagine a DocumentCollection? Corpus? class that would represent
    // a set of documents/files ....
    Vector x_vec = Node.getVectorFromMap(x_nodes);
    neurogridnetwork.generateRingTopology(x_vec,2);
    // so what is the default max connectivity? seems to be 3 --> should ensure that
    
    SearchStatistics x_search_stats = null;
    Document x_doc = null;
    Node x_start_node = null;
    int x_TTL = 5;
    int[] x_stats = new int[6];    
    x_stats[ACTIVATED_NODES] = 0;
    x_stats[POSSIBLE_TARGETS] = 0;
    x_stats[MESSAGE_TRANSFERS] = 0;
    x_stats[TTL_FIRST_MATCH] = -1;
    x_stats[NO_MATCHES] = 0;
    x_stats[NO_FALSE_MATCHES] = 0;
    HashSet x_checked_nodes = new HashSet(); 
    HashSet x_possible_targets = new HashSet(); 
    LinkedList x_unchecked_nodes = new LinkedList(); 
    ContentMessage x_message = null;
        
    // need to work out how to turn off most of the logging and loop this
    // FIXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX    
        
    while(x_doc_iter.hasNext())    
    {
      // would be nice to be able to choose the type of node we start searching from ...	
      x_doc = (Document)x_doc_iter.next();
      //x_node = (Node)x_node_iter.next();
      x_start_node = (Node)neurogridnetwork.getRandomSearchStart(new HashSet(),
                                                           "com.neurogrid.simulation.NeuroGridNode",
                                                           x_random);
      x_message = new SimpleContentMessage(x_TTL, 
                                x_doc.getKeywords(), 
                                x_doc,
                                x_start_node);
                                
      x_search_stats = neurogridnetwork.searchNetwork(x_message);

      // seems to require me to have a store of which items to search
      // but also to associate some flag with each of them to indicate 
      // whether something was discovered above it in the search tree                                              
                                             
      System.out.println("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!starting node!!!!!!!!!!!!!!!!!!!!!!!!!!!!!: "+x_start_node.getNodeID());
      /*
                
      checkNode(new SearchHolder(x_node,
                                 x_node,
                                 x_doc,
                                 x_doc.getKeywords(),
                                 0,
                                 x_TTL,
                                 x_stats,
                                 x_checked_nodes,
                                 x_possible_targets,
                                 false,
                                 x_unchecked_nodes));
      
      SearchHolder x_holder = null;
      
      do
      {
        x_holder = (SearchHolder)(x_unchecked_nodes.removeLast());

        System.out.println("x_unchecked_nodes.size(): "+x_unchecked_nodes.size());
        System.out.println("x_node: "+x_holder.o_node.getNodeID());
        
        checkNode(x_holder);
                     
      }while(x_unchecked_nodes.size() > 0);
      
        System.out.println("x_stats[ACTIVATED_NODES]: "+x_stats[ACTIVATED_NODES]);
        System.out.println("x_stats[POSSIBLE_TARGETS]: "+x_stats[POSSIBLE_TARGETS]);
        System.out.println("x_stats[MESSAGE_TRANSFERS]: "+x_stats[MESSAGE_TRANSFERS]);
        System.out.println("x_stats[TTL_FIRST_MATCH]: "+x_stats[TTL_FIRST_MATCH]);
        System.out.println("x_stats[NO_MATCHES]: "+x_stats[NO_MATCHES]);
        System.out.println("x_stats[NO_FALSE_MATCHES]: "+x_stats[NO_FALSE_MATCHES]);
      */
      
      //assertTrue("No. activated nodes incorrect",x_search_stats.getNoActivatedNodes()==x_stats[ACTIVATED_NODES]);
      //assertTrue("No. possible targets incorrect",x_search_stats.getNoPossibleTargets()==x_stats[POSSIBLE_TARGETS]);
      //assertTrue("No. message transfers incorrect",x_search_stats.getNoMessageTransfers()==x_stats[MESSAGE_TRANSFERS]);

      //assertTrue("first match ttl incorrect",x_search_stats.getTTLOfFirstMatch()==x_stats[TTL_FIRST_MATCH]);
      //assertTrue("No. matches incorrect",x_search_stats.getNoMatches()==x_stats[NO_MATCHES]);
      //assertTrue("No. false matches incorrect",x_search_stats.getNoFalseMatches()==x_stats[NO_FALSE_MATCHES]);

      // don't know how to predict the above for the NG networks
      // FIXXXXXXXXXXXXXXXXXXXXXXXXX

  
      // makes more sense to do refresh test here ...
      
      neurogridnetwork.refresh();
    }
      // if learning is taking place then we can't rely on a second run being identical unless we
      // clear knowledge bases and connections ...
      /*
      Iterator x_iter = neurogridnetwork.getNodes().values().iterator();
      Node x_node = null;
      while(x_iter.hasNext())
      {
        x_node = (Node)(x_iter.next());
        x_node.clearKnowledge();
        x_node.clearConnList();
      }
      
      neurogridnetwork.generateRingTopology(x_vec,2);     
           
      x_message = new SimpleMessage(x_TTL, 
                                x_doc.getKeywords(), 
                                x_doc,
                                x_start_node);
                                      
      SearchStatistics x_search_stats2 = neurogridnetwork.searchNetwork(x_message);

      // would love to check how many objects we were increasing by after each loop
      


 
      assertTrue("post-refresh: No. activated nodes incorrect",
                 x_search_stats.getNoActivatedNodes()==x_search_stats2.getNoActivatedNodes());
      assertTrue("post-refresh: No. possible targets incorrect",
                 x_search_stats.getNoPossibleTargets()==x_search_stats2.getNoPossibleTargets());
      assertTrue("post-refresh: No. message transfers incorrect",
                 x_search_stats.getNoMessageTransfers()==x_search_stats2.getNoMessageTransfers());
      // the above is not consistent due to a failure to handle randomness?
      // FIXXXXXXXXXXXXXX
      // I suspect that there is unpredictability in the way NeuroGrid handles forwarding ...           
                 
      assertTrue("post-refresh: first match ttl incorrect",
                 x_search_stats.getTTLOfFirstMatch()==x_search_stats2.getTTLOfFirstMatch());
      assertTrue("post-refresh: No. matches incorrect",
                 x_search_stats.getNoMatches()==x_search_stats2.getNoMatches());
      assertTrue("post-refresh: No. false matches incorrect",
                 x_search_stats.getNoFalseMatches()==x_search_stats2.getNoFalseMatches());
                 
      assertTrue("start node failed to learn anything",
                 x_start_node.getTotalNoKnowledge() != 0);
     // assertTrue("start node failed to increase connectivity",
       //          x_start_node.getNoConnections() > 2);
      //connectivity will fail to increase if the target is found in nodes we are already connected to ...
                 
*/
 
      // so we should be checking the stats coming back from the search ...
      /*
      x_search_stats.getNoMatches()
      x_search_stats.getTTLOfFirstMatch()
      x_search_stats.getNoMessageTransfers()
      x_search_stats.getNoActivatedNodes()
      x_search_stats.getNoPossibleTargets() */
    //}
    
    
    // would be nice to back all of this up to cvs - or even to the linux box ...
    
    return x_search_stats;
    
    // JUnitDoclet end method searchNetwork
  }
  
  class SearchHolder
  {
    Node o_node = null;
    Node o_transmitter = null;
    Document o_doc = null;
    Keyword[] o_keywords = null;
    int o_depth = -1;
    int o_TTL = -1;
    int[] o_stats = null;
    HashSet o_nodes = null;
    HashSet o_documents = null;
    boolean o_found = false;
    LinkedList o_unchecked_nodes = null;
    
    SearchHolder(Node p_node,
                 Node p_transmitter,
                 Document p_doc,
                 Keyword[] p_keywords,
                 int p_depth,
                 int p_TTL,
                 int[] p_stats,
                 HashSet p_nodes,
                 HashSet p_documents,
                 boolean p_found,
                 LinkedList p_unchecked_nodes)
    {
      o_node = p_node;
      o_transmitter = p_transmitter;
      o_doc = p_doc;
      o_keywords = p_keywords;
      o_depth = p_depth;
      o_TTL = p_TTL;
      o_stats = p_stats;
      o_nodes = p_nodes;
      o_documents = p_documents;
      o_found = p_found;
      o_unchecked_nodes = p_unchecked_nodes;	
    }
    
  }
  
  private final static int ACTIVATED_NODES = 0;
  private final static int POSSIBLE_TARGETS = 1;
  private final static int MESSAGE_TRANSFERS = 2;
  private final static int TTL_FIRST_MATCH = 3;
  private final static int NO_MATCHES = 4;
  private final static int NO_FALSE_MATCHES = 5;
    
  private void checkNode(SearchHolder x_holder)
    throws Exception
  {  
    // check the activation as it spreads out from start node     
    System.out.println("checking node: "+x_holder.o_node.getNodeID());
    System.out.println("x_holder.o_depth: "+x_holder.o_depth);
    System.out.println("x_holder.o_TTL: "+x_holder.o_TTL);        
    System.out.println("x_holder.o_found: "+x_holder.o_found); 
      	
    if(x_holder.o_depth <= x_holder.o_TTL && x_holder.o_depth != 0)// && x_holder.o_found == false)
    {
      x_holder.o_stats[MESSAGE_TRANSFERS]++;     
      // problem here is that network avoids sending back down connection
      // message received from ....
    }
      	  
    if(x_holder.o_nodes.contains(x_holder.o_node))
    {
      return;	
    }
    else
    {
      x_holder.o_nodes.add(x_holder.o_node);	
    }
     
    if(x_holder.o_depth < x_holder.o_TTL)// && x_holder.o_found == false)
    {
      assertTrue("Connected node is not active - "+x_holder.o_node.getNodeID(),x_holder.o_node.getActive());
    }
    else
    {
      assertTrue("Connected node passed TTL or after discovery is active - "+x_holder.o_node.getNodeID(),
                 !x_holder.o_node.getActive());           
    }
    if(x_holder.o_node.getActive())
    {
      x_holder.o_stats[ACTIVATED_NODES]++;	
    }
      	
    System.out.println("Checking keywords in: " + x_holder.o_node.getNodeID());
      	
    Keyword[] x_keywords = x_holder.o_node.allKeywords();
    for(int k=0;k<x_keywords.length;k++)
    {
      System.out.println("x_keywords["+k+"]"+x_keywords[k]);
    }
    System.out.println("x_holder.o_node.matchingKeywords(x_holder.o_keywords): "+
    x_holder.o_node.matchingKeywords(x_holder.o_keywords));
    System.out.println((x_holder.o_doc == null && x_holder.o_node.matchingKeywords(x_holder.o_keywords)>0));
    System.out.println(x_holder.o_node.matchingKeywords(x_holder.o_keywords)>0);
    System.out.println(x_holder.o_doc == null);
    for(int k=0;k<x_holder.o_keywords.length;k++)
    {
      System.out.println("x_keywords["+k+"]"+x_holder.o_keywords[k]);
    }
    if(x_holder.o_node.hasContent(x_holder.o_doc)) // this is different for fuzzy saerches
    {
      System.out.println("Found target in: " + x_holder.o_node.getNodeID());

      x_holder.o_stats[POSSIBLE_TARGETS]++;	    
      	    
      if(x_holder.o_depth > 0)// not sure about this - takes care of no local search at start ...
      {
        if(x_holder.o_depth < x_holder.o_TTL)// && x_holder.o_found == false) //remnant from stops when match 
          x_holder.o_stats[NO_MATCHES]++;

        x_holder.o_found = true;            
      	  
      	if(x_holder.o_stats[TTL_FIRST_MATCH] == -1)
      	{
      	  System.out.println("first match: "+x_holder.o_node.getNodeID()+", x_holder.o_TTL - x_holder.o_depth = " +(x_holder.o_TTL - x_holder.o_depth));	
      	  x_holder.o_stats[TTL_FIRST_MATCH] = x_holder.o_TTL - x_holder.o_depth;
      	}
      }
    }
    System.out.println("x_stats[ACTIVATED_NODES]: "+x_holder.o_stats[ACTIVATED_NODES]);
    System.out.println("x_stats[POSSIBLE_TARGETS]: "+x_holder.o_stats[POSSIBLE_TARGETS]);
    System.out.println("x_stats[MESSAGE_TRANSFERS]: "+x_holder.o_stats[MESSAGE_TRANSFERS]);
    System.out.println("x_stats[TTL_FIRST_MATCH]: "+x_holder.o_stats[TTL_FIRST_MATCH]);
    System.out.println("x_stats[NO_MATCHES]: "+x_holder.o_stats[NO_MATCHES]);
    System.out.println("x_stats[NO_FALSE_MATCHES]: "+x_holder.o_stats[NO_FALSE_MATCHES]);

    Node x_conn = null;
        
    Collection x_coll = x_holder.o_node.getConnList().values();
    x_coll.remove(x_holder.o_transmitter); // avoid checking the node we received from ...
    System.out.println("removing: " +x_holder.o_transmitter.getNodeID());
       
    Iterator x_conn_iter = x_coll.iterator();
        
    while(x_conn_iter.hasNext())                                   
    {
      x_conn = (Node)(x_conn_iter.next());     
      x_holder.o_unchecked_nodes.addFirst(new SearchHolder(x_conn,
   	                                                   x_holder.o_node,
                                                           x_holder.o_doc,
                                                           x_holder.o_keywords,
                                                           x_holder.o_depth+1,
                                                           x_holder.o_TTL,
                                                           x_holder.o_stats,
                                                           x_holder.o_nodes,
                                                           x_holder.o_documents,
                                                           x_holder.o_found,
                                                           x_holder.o_unchecked_nodes));
    }
  }
    
  private void checkFuzzyNode(SearchHolder x_holder)
    throws Exception
  {  
    // check the activation as it spreads out from start node     
    System.out.println("checking node: "+x_holder.o_node.getNodeID());
    System.out.println("x_holder.o_depth: "+x_holder.o_depth);
    System.out.println("x_holder.o_TTL: "+x_holder.o_TTL);        
    System.out.println("x_holder.o_found: "+x_holder.o_found); 
      	
    if(x_holder.o_depth <= x_holder.o_TTL && x_holder.o_depth != 0)// && x_holder.o_found == false )
    {
      x_holder.o_stats[MESSAGE_TRANSFERS]++;     
      // problem here is that network avoids sending back down connection
      // message received from ....
    }
      	  
    if(x_holder.o_nodes.contains(x_holder.o_node))
    {
      return;	
    }
    else
    {
      x_holder.o_nodes.add(x_holder.o_node);	
    }
     
    if(x_holder.o_depth < x_holder.o_TTL)// && x_holder.o_found == false)
    {
      assertTrue("Connected node is not active - "+x_holder.o_node.getNodeID(),x_holder.o_node.getActive());
    }
    else
    {
      assertTrue("Connected node passed TTL or after discovery is active - "+x_holder.o_node.getNodeID(),
                 !x_holder.o_node.getActive());           
    }
    if(x_holder.o_node.getActive())
    {
      x_holder.o_stats[ACTIVATED_NODES]++;	
    }
      	
    System.out.println("Checking keywords in: " + x_holder.o_node.getNodeID());
      	
    Keyword[] x_keywords = x_holder.o_node.allKeywords();
    for(int k=0;k<x_keywords.length;k++)
    {
      System.out.println("x_keywords["+k+"]"+x_keywords[k]);
    }
    System.out.println("x_holder.o_node.matchingKeywords(x_holder.o_keywords): "+
    x_holder.o_node.matchingKeywords(x_holder.o_keywords));
    System.out.println((x_holder.o_doc == null && x_holder.o_node.matchingKeywords(x_holder.o_keywords)>0));
    System.out.println(x_holder.o_node.matchingKeywords(x_holder.o_keywords)>0);
    System.out.println(x_holder.o_doc == null);
    for(int k=0;k<x_holder.o_keywords.length;k++)
    {
      System.out.println("x_keywords["+k+"]"+x_holder.o_keywords[k]);
    }
    
    if(x_holder.o_node.matchingKeywords(x_holder.o_keywords)>0) // this is different for fuzzy saerches
    {
      System.out.println("Found target in: " + x_holder.o_node.getNodeID());

      //x_holder.o_possible_targets.addAll(x_holder.o_node.matchingDocuments(x_holder.o_keywords));
      x_holder.o_stats[POSSIBLE_TARGETS]+=x_holder.o_node.matchingDocuments(x_holder.o_keywords).size();	    
      	    
      if(x_holder.o_depth > 0)// not sure about this - takes care of no local search at start ...
      {
        if(x_holder.o_depth < x_holder.o_TTL)// && x_holder.o_found == false)  
          x_holder.o_stats[NO_MATCHES]+=x_holder.o_node.matchingDocuments(x_holder.o_keywords).size();

        x_holder.o_found = true;            
      	  
      	if(x_holder.o_stats[TTL_FIRST_MATCH] == -1)
      	{
      	  System.out.println("first match: "+x_holder.o_node.getNodeID()+", x_holder.o_TTL - x_holder.o_depth = " +(x_holder.o_TTL - x_holder.o_depth));	
      	  x_holder.o_stats[TTL_FIRST_MATCH] = x_holder.o_TTL - x_holder.o_depth;
      	}
      }
    }
    System.out.println("x_stats[ACTIVATED_NODES]: "+x_holder.o_stats[ACTIVATED_NODES]);
    System.out.println("x_stats[POSSIBLE_TARGETS]: "+x_holder.o_stats[POSSIBLE_TARGETS]);
    System.out.println("x_stats[MESSAGE_TRANSFERS]: "+x_holder.o_stats[MESSAGE_TRANSFERS]);
    System.out.println("x_stats[TTL_FIRST_MATCH]: "+x_holder.o_stats[TTL_FIRST_MATCH]);
    System.out.println("x_stats[NO_MATCHES]: "+x_holder.o_stats[NO_MATCHES]);
    System.out.println("x_stats[NO_FALSE_MATCHES]: "+x_holder.o_stats[NO_FALSE_MATCHES]);

    Node x_conn = null;
        
    Collection x_coll = x_holder.o_node.getConnList().values();
    x_coll.remove(x_holder.o_transmitter); // avoid checking the node we received from ...
    System.out.println("removing: " +x_holder.o_transmitter.getNodeID());
       
    Iterator x_conn_iter = x_coll.iterator();
        
    while(x_conn_iter.hasNext())                                   
    {
      x_conn = (Node)(x_conn_iter.next());     
      x_holder.o_unchecked_nodes.addFirst(new SearchHolder(x_conn,
   	                                                   x_holder.o_node,
                                                           x_holder.o_doc,
                                                           x_holder.o_keywords,
                                                           x_holder.o_depth+1,
                                                           x_holder.o_TTL,
                                                           x_holder.o_stats,
                                                           x_holder.o_nodes,
                                                           x_holder.o_documents,
                                                           x_holder.o_found,
                                                           x_holder.o_unchecked_nodes));
    }
  }


  
  public void testCreateDocuments() throws Exception {
    // JUnitDoclet begin method createDocuments
    
    // so far I have only looked at creating uniform and zipf distribution of keywords over documents
    // in a zipf distribution we give over control of how many documents we have, but in a uniform 
    // distribution we can specify a certain amount of keywords to create different effects ...

    neurogridnetwork.createKeywords(5);
    int x_no_docs = 5;
    int x_no_keywords_per_doc = 3;
    
    // feels like we should be specifying the type of distribution through an argument rather than
    // a global variable
    // also feels like we should be handing back a set of documents to check ....
    
    HashMap x_docs = neurogridnetwork.createDocuments(x_no_docs,
                                                     x_no_keywords_per_doc,
                                                     false,
                                                     new Random());
                                                               
    assertTrue("Wrong Number of docs",x_docs.size() == x_no_docs);
    Iterator x_iter = x_docs.values().iterator();
    Document x_doc = null;
    while(x_iter.hasNext())
    {
      x_doc = (Document)(x_iter.next());
      assertTrue("Wrong number of keywords per doc",x_doc.getKeywords().length == x_no_keywords_per_doc);
      for(int j=0;j<x_no_keywords_per_doc;j++)
      {
      	assertTrue(x_doc.getKeywords()[j].getClass().getName().equals("com.neurogrid.simulation.SimpleKeyword"));
      }
    }
    
    // JUnitDoclet end method createDocuments
  }

  public void testGenerateContent() throws Exception {
    // JUnitDoclet begin method generateContent
    
    HashMap x_map = neurogridnetwork.createNodes(5,5,new Random(888));
    neurogridnetwork.generateContent(x_map,
                                   Document.o_documents,
                                   Document.o_document_ids,
                                   3,
                                   false,
                                   new Random());
    Node x_node = null;
    
    Iterator x_iter = x_map.values().iterator();
    while(x_iter.hasNext())
    {
      x_node = (Node)(x_iter.next());
      assertTrue(x_node.getNoContents()==3);
      for(int j=0;j<3;j++)
      {
      	assertTrue(x_node.getContentsByDocID().values().toArray()[j].getClass().getName().equals("com.neurogrid.simulation.SimpleDocument"));
      }	
    }
    
    
    // JUnitDoclet end method generateContent
  }
  
  public void testGenerateConnections() throws Exception {
    // JUnitDoclet begin method generateConnections
    
    
    
    // JUnitDoclet end method generateConnections
  }
  
  
  public void testGenerateRingTopology() throws Exception {
    // JUnitDoclet begin method generateRingTopology
   
    int x_no_nodes = 5;
    int x_no_conns = 2;
    
    neurogridnetwork.o_max_connections_per_node=4;
    HashMap x_map = neurogridnetwork.createNodes(x_no_nodes,x_no_nodes,new Random(888));
    Vector x_vec = Node.getVectorFromMap(x_map);
    neurogridnetwork.generateRingTopology(x_vec,x_no_conns);
    
    Node x_previous_node = null;
    Node x_node = null;
    Node x_next_node = null;
    
    for(int i=0;i<x_no_nodes;i++)
    {
    	// maybe this should be calling a getNode(Node p_node) method ...? //FIXXXXXXXXXX
      x_previous_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt((i-1+x_no_nodes)%x_no_nodes)));
      x_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt(i)));
      x_next_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt((i+1)%x_no_nodes)));
      assertTrue("no connection to previous node",x_node.hasConnection(x_previous_node));
      assertTrue("no connection to next node",x_node.hasConnection(x_next_node));
    }
 
    x_no_nodes = 10;
    x_no_conns = 4;
    
   
    x_map = neurogridnetwork.createNodes(x_no_nodes,x_no_nodes,new Random(888));
    x_vec = Node.getVectorFromMap(x_map);
    neurogridnetwork.generateRingTopology(x_vec,x_no_conns);
    
    Node x_previous_previous_node = null;
    x_previous_node = null;
    x_node = null;
    x_next_node = null;
    Node x_next_next_node = null;
    
    for(int i=0;i<5;i++)
    {
    	// maybe this should be calling a getNode(Node p_node) method ...? //FIXXXXXXXXXX
      x_previous_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt((i-1+x_no_nodes)%x_no_nodes)));
      x_previous_previous_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt((i-2+x_no_nodes)%x_no_nodes)));
      x_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt(i)));
      x_next_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt((i+1)%x_no_nodes)));
      x_next_next_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt((i+2)%x_no_nodes)));
      assertTrue("no connection to previous previous node",x_node.hasConnection(x_previous_previous_node));
      assertTrue("no connection to previous node",x_node.hasConnection(x_previous_node));
      assertTrue("no connection to next node",x_node.hasConnection(x_next_node));
      assertTrue("no connection to next next node",x_node.hasConnection(x_next_next_node));
    }
    
    
    // JUnitDoclet end method generateRingTopology
  }
  
  public void testGenerateRandomTopology() throws Exception {
    // JUnitDoclet begin method generateRingTopology
    
    int x_no_nodes = 5;
    int x_no_conns = 2;
    
    HashMap x_map = neurogridnetwork.createNodes(x_no_nodes,x_no_nodes,new Random(888));
    Vector x_vec = Node.getVectorFromMap(x_map);
    neurogridnetwork.generateRandomTopology(x_vec,x_vec,x_no_conns,false);
    // should be testing higher degrees of connectivity ....FIXXXXXXXXX
    // should be testing reciprocal connection making ... FIXXXXXXXXXXXXX
    
    Node x_previous_node = null;
    Node x_node = null;
    Node x_next_node = null;
    
    for(int i=0;i<x_no_nodes;i++)
    {
      x_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt(i)));
      assertTrue("more than 2 connections per node, i.e. "+x_node.getNoConnections(),
                 x_node.getNoConnections() == x_no_conns);      
      for(int j=0;j<x_no_conns;j++)
      {
      	assertTrue("Connection not to specified set of nodes",
      	           x_map.containsKey(x_node.getConnList().keySet().toArray()[j]));
      }
    }
    
    x_no_nodes = 10;
    x_no_conns = 3;
    
    x_map = neurogridnetwork.createNodes(x_no_nodes,x_no_nodes,new Random(888));
    x_vec = Node.getVectorFromMap(x_map);
    neurogridnetwork.generateRandomTopology(x_vec,x_vec,x_no_conns,true);
    // should be testing higher degrees of connectivity ....FIXXXXXXXXX
    // should be testing reciprocal connection making ... FIXXXXXXXXXXXXX
    
    x_previous_node = null;
    x_node = null;
    x_next_node = null;
    
    for(int i=0;i<x_no_nodes;i++)
    {
      x_node = (Node)(neurogridnetwork.getNodes().get(x_vec.elementAt(i)));
      // not sure what the real limit is on connectivity when we make connections reciprocal ...
      //System.out.println(x_node.getNoConnections());
      //assertTrue("more than 2 connections per node, i.e. "+x_node.getNoConnections(),
      //           x_node.getNoConnections() == 2);
      for(int j=0;j<neurogridnetwork.o_max_connections_per_node;j++)
      {
      	assertTrue("Connection not to specified set of nodes",
      	           x_map.containsKey(x_node.getConnList().keySet().toArray()[j]));
      }
    }
    
    
    // JUnitDoclet end method generateRingTopology
  }
  
  public void testGenerateTwoWayConnections() throws Exception {
    // JUnitDoclet begin method generateTwoWayConnections
    // JUnitDoclet end method generateTwoWayConnections
  }
  
  public void testGetUniformDistribution() throws Exception {
    // JUnitDoclet begin method getUniformDistribution
    
    // need to make sure there are actually some keywords
    neurogridnetwork.createKeywords(5);
    int x_no_bins = 5;
    int x_no_items_per_bin = 3;
    HashMap x_item_table = Keyword.o_keywords;
    HashMap x_item_id_table = Keyword.o_keywords;
    
    // dist works over all the keywords - not just the ones we create here ...
    // other approaches could involve fixing the number of each keyword present
    // in advance, to remove random factor ...
    
    Object[][] x_dist = neurogridnetwork.getUniformDistribution(x_no_bins,
                                                               x_no_items_per_bin,
                                                               x_item_table,
                                                               x_item_id_table,
                                                               new Random());
                                                               
    // definitely some sort of problem here since all documents 
    // are coming back with the same keywords - FIXXXXXXXXXXXXXXXX
    // need to create a test that will sort this ...           
    
    // would be good to be fixing random seeds so we can replicate ...              
                                                               
    assertTrue(x_dist.length == x_no_bins);
    int k=0;
    for(int i=0;i<x_no_bins;i++)
    {
      assertTrue(x_dist[i].length == x_no_items_per_bin);
      for(int j=0;j<x_no_items_per_bin;j++)
      {
      	if(x_dist[i][j] == x_dist[(i+1)%x_no_bins][j]) k++;
      	
      //	assertTrue("Same keyword in same place x_dist["+i+"]["+j+"]:" + 
      //	           x_dist[i][j] + " != x_dist["+(i+1)%x_no_bins+"]["+j+"]:"+ x_dist[(i+1)%x_no_bins][j],
      //	           x_dist[i][j] != x_dist[(i+1)%x_no_bins][j]);
      	assertTrue("Items not SimpleKeywords",x_dist[i][j].getClass().getName().equals("com.neurogrid.simulation.SimpleKeyword"));
      	System.out.print(x_dist[i][j] + " ");
      }
      System.out.println();
    }

    assertTrue("Suspiciously high number of similar items",k<(x_no_bins*x_no_items_per_bin/2));
    
    // not sure how to write a test that wil check the distribution
    // I guess we need some kind of metric such as the number of times that each 
    // item occurs - even then randomness could cause a test to fail ...
    // I guess we would have to run something many many times and then 
    // we woudl get an assessment of the effective randomness etc ...
    
    // JUnitDoclet end method getUniformDistribution
  }
  
  public void testGetZipfDistribution() throws Exception {
    // JUnitDoclet begin method getZipfDistribution
    // JUnitDoclet end method getZipfDistribution
  }
  
  

  /**
  * JUnitDoclet moves marker to this method, if there is not match
  * for them in the regenerated code and if the marker is not empty.
  * This way, no test gets lost when regenerating after renaming.
  * Method testVault is supposed to be empty.
  */
  public void testVault() throws Exception {
    // JUnitDoclet begin method testcase.testVault
    // JUnitDoclet end method testcase.testVault
  }
  
  public static void main(String[] args) {
    // JUnitDoclet begin method testcase.main
    String x_method = System.getProperty("test.method");
    System.out.println("test.method="+x_method);
    if(x_method == null || x_method.equals(""))
    {
      System.out.println("testing all methods");
    
      junit.textui.TestRunner.run(NeuroGridNetworkTest.class);
    }
    else
    {
      System.out.println("testing single method");

      TestSuite suite = new TestSuite();
      suite.addTest(new NeuroGridNetworkTest(x_method));
      junit.textui.TestRunner.run(suite);
    }
    // JUnitDoclet end method testcase.main
  }
  
}
